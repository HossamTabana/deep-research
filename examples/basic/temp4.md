yarn dev
yarn run v1.22.22
$ tsx examples/basic/simple-research.ts
Starting deep research...
[Step 1] Generating research plan... at 1
expected output tokens: 1500
[Step 2] Running initial web searches with 3 queries...
Received 3 initial search results
[Step 2.5] Deduplicating search results...
[Step 3] Reasoning about the search results...
[Step 4] Decision making...
[Step 1] Generating research plan... at 2
expected output tokens: undefined
[Step 2] Running initial web searches with 5 queries...
Received 5 initial search results
[Step 2.5] Deduplicating search results...
[Step 3] Reasoning about the search results...
[Step 4] Decision making...
[Step 1] Generating research plan... at 3
expected output tokens: undefined
[Step 2] Running initial web searches with 7 queries...
Received 7 initial search results
[Step 2.5] Deduplicating search results...
[Step 3] Reasoning about the search results...
[Step 4] Decision making...
[Step 5] Generating report...
[Iteration 0] phase=initial
[Iteration 1] phase=continuation
[Iteration 2] phase=continuation
[Iteration 3] phase=continuation
[Iteration 4] phase=continuation
[Iteration 5] phase=continuation
[Iteration 6] phase=continuation
Reference map size: 111
Generating bibliography with 111 entries
Done processing report for sources
====
result The PARROT-360V benchmark includes 2,487 visual puzzles designed to test Vision Language Models (VLMs) on complex visual reasoning tasks [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)]. This benchmark is designed to evaluate the performance of VLMs using visual puzzles [[35](https://desolarplace.com/product/su-kam-65kva-360v-3-phase-colossal-inverter-2/)]. PARROT-360V is a novel benchmark designed to evaluate the ability of Vision Language Models (VLMs) to understand and process complex visual and textual information [[8](https://www.redblock.ai/blog/parrot-360v-elevating-vlm-evaluation-through-real-world-problem-solving), [10](https://www.pictorem.com/456324/a-parot/), [19](https://www.dataparrot.ai), [20](https://huggingface.co/datasets/AIDC-AI/Parrot-dataset), [21](https://www.mojeek.com/), [22](https://idptools-parrot.readthedocs.io/en/stable/usage/basic_examples.html), [23](https://catalog.data.gov/dataset/?tags=par), [24](https://arxiv.org/abs/2011.10024), [25](https://github.com/Akaneqwq/360DVD), [26](https://arxiv.org/html/2407.14066v1), [27](https://arxiv.org/abs/2001.02152), [28](https://www.researchgate.net/figure/Dataset-of-26-Amazon-parrot-species-examined-in-this-study_tbl1_363216475), [29](https://github.com/jamycheung/360BEV/blob/main/README.md), [30](https://github.com/jamycheung/360BEV), [31](https://github.com/katherinethai/par3), [32](https://fr.linkedin.com/in/sophie-parot-634508152), [33](https://annuaire-entreprises.data.gouv.fr/etablissement/34921482500037), [34](https://annuaire-entreprises.data.gouv.fr/entreprise/349214825), [40](https://www.parrot.com/en), [41](https://www.parrot.com/us/drones/anafi/technical-specifications), [42](https://desolarplace.com/product/su-kam-40kva-360v-3-phase-colossal-inverter/), [43](https://citysolarmall.com/product/su-kam-50kva-360v-3-phase-colossal-inverter/), [44](https://www.parrot.com/us/drones/anafi-usa/technical-specifications), [45](https://www.manualslib.com/manual/1409899/Growatt-1500-Us.html?page=28), [46](https://www.parrot.com/us/drones/anafi-thermal/technical-specifications), [47](https://fr.linkedin.com/in/caroline-parot-72714252), [48](https://www.by-parot.com/), [49](https://avis-deces.linternaute.com/nom/famille-parot), [50](https://openreview.net/forum?id=Vx1JadlOIt), [51](https://huggingface.co/blog/vlms), [52](https://encord.com/blog/vision-language-models-guide/), [53](https://huggingface.co/blog/vlms-2025), [54](https://openreview.net/pdf?id=Vx1JadlOIt), [55](https://www.nature.com/articles/s41586-024-08378-w), [56](https://ieeexplore.ieee.org/document/10445007/), [57](https://www.nvidia.com/en-us/glossary/vision-language-models/), [58](https://arxiv.org/abs/2501.02189), [59](https://arxiv.org/abs/2304.00685), [60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3), [61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks), [62](https://arxiv.org/abs/2305.11175), [63](https://github.com/jingyi0000/VLM_survey), [64](https://www.ibm.com/think/topics/vision-language-models), [65](https://arxiv.org/html/2401.01862v1), [66](https://arxiv.org/abs/2505.11838), [67](https://escholarship.org/uc/item/3tc6256x), [68](https://paperswithcode.com/task/visual-reasoning/latest), [69](https://practicalpie.com/perceptual-reasoning/), [70](https://visualcommonsense.com/), [71](https://paperswithcode.com/task/visual-commonsense-reasoning), [72](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms), [73](https://nihtoolbox.org/test/visual-reasoning-test/), [74](https://www.continued.com/early-childhood-education/ask-the-experts/what-are-visual-perceptual-skills-22984), [75](http://brainy-child.com/experts/WISC-IV-perceptual-reasoning.shtml), [76](https://github.com/atfortes/Awesome-LLM-Reasoning), [77](https://iclr.cc/virtual/2025/calendar), [78](https://link.springer.com/article/10.1007/s10489-025-06325-4), [79](https://paperswithcode.com/dataset/vcr), [80](https://huggingface.co/tasks/visual-question-answering), [81](https://www.bloomberg.com/news/articles/2025-04-16/openai-releases-new-reasoning-models-for-coding-and-visual-tasks), [82](https://www.parrotanalytics.com/), [83](https://universe.roboflow.com/fish-zqrse/parrot-fish-nq63l), [84](https://arxiv.org/html/2407.14066v3), [85](https://www.marketscreener.com/quote/stock/GROUPE-PAROT-31670658/company/), [86](https://www.cpubenchmark.net/singleCompare.php?remove=3859), [87](https://360rumors.com/360-camera-comparison-tool/), [88](https://www.videocardbenchmark.net), [89](https://videocardbenchmark.net/singleCompare.php?redirect=), [90](https://cyberforge.academy/parrot-os-vs-kali-linux-the-ultimate-comparison-guide/), [91](https://www.lebloginsta360.com/en/blog/tags/360-degree-camera-comparison), [92](https://pitchbook.com/profiles/company/161768-35), [93](https://www.cpubenchmark.net/), [94](https://panoraven.com/blog/en/360-best-cameras/), [95](https://tweakers.net/best-buy-guide/processors/benchmarks), [96](https://www.parrot.com/en/apps-and-services), [97](https://x.com/pro360vision), [98](https://dl.acm.org/doi/10.1609/aaai.v37i3.25450), [99](https://deepai.org/publication/parot-a-practical-framework-for-robust-deep-neuralnetwork-training), [100](https://energyconnections.net.au/products/noark-dual-pole-dc-circuit-breaker-360v), [101](https://www.mouser.mx/fairchildpdpigbt/), [102](https://www.onworks.net/software/windows/app-parot-polar-aligner), [103](https://www.linkedin.com/in/john-bryan-parot-051a88133/), [104](https://www.academia.edu/42057304), [105](https://dl.acm.org/doi/abs/10.1016/j.eswa.2021.114712), [106](https://www.dronenerds.com/collections/drones-enterprise-drones-parrot-business), [107](https://www.sciencedirect.com/science/article/pii/S2590188520300032), [108](https://www.parrotsoftware.com/), [109](https://fr.wikipedia.org/wiki/Parrot_(entreprise)), [110](https://partav.com/), [111](https://www.usine-digitale.fr/parrot/)].The PARROT-360V benchmark is designed to evaluate Vision Language Models (VLMs) in complex, real-world problem-solving scenarios [[50](https://openreview.net/forum?id=Vx1JadlOIt)][[51](https://huggingface.co/blog/vlms)][[52](https://encord.com/blog/vision-language-models-guide/)][[53](https://huggingface.co/blog/vlms-2025)][[54](https://openreview.net/pdf?id=Vx1JadlOIt)][[55](https://www.nature.com/articles/s41586-024-08378-w)][[56](https://ieeexplore.ieee.org/document/10445007/)][[57](https://www.nvidia.com/en-us/glossary/vision-language-models/)][[58](https://arxiv.org/abs/2501.02189)][[59](https://arxiv.org/abs/2304.00685)][[60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3)][[61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks)][[62](https://arxiv.org/abs/2305.11175)][[63](https://github.com/jingyi0000/VLM_survey)][[64](https://www.ibm.com/think/topics/vision-language-models)][[65](https://arxiv.org/html/2401.01862v1)]. It includes 2,487 visual puzzles designed to test VLMs on complex visual reasoning tasks [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)]. This benchmark evaluates the performance of VLMs using visual puzzles [[35](https://desolarplace.com/product/su-kam-65kva-360v-3-phase-colossal-inverter-2/)]. PARROT-360V assesses the ability of VLMs to understand and process complex visual and textual information [[8](https://www.redblock.ai/blog/parrot-360v-elevating-vlm-evaluation-through-real-world-problem-solving), [10](https://www.pictorem.com/456324/a-parot/), [19](https://www.dataparrot.ai), [20](https://huggingface.co/datasets/AIDC-AI/Parrot-dataset), [21](https://www.mojeek.com/), [22](https://idptools-parrot.readthedocs.io/en/stable/usage/basic_examples.html), [23](https://catalog.data.gov/dataset/?tags=par), [24](https://arxiv.org/abs/2011.10024), [25](https://github.com/Akaneqwq/360DVD), [26](https://arxiv.org/html/2407.14066v1), [27](https://arxiv.org/abs/2001.02152), [28](https://www.researchgate.net/figure/Dataset-of-26-Amazon-parrot-species-examined-in-this-study_tbl1_363216475), [29](https://github.com/jamycheung/360BEV/blob/main/README.md), [30](https://github.com/jamycheung/360BEV), [31](https://github.com/katherinethai/par3), [32](https://fr.linkedin.com/in/sophie-parot-634508152), [33](https://annuaire-entreprises.data.gouv.fr/etablissement/34921482500037), [34](https://annuaire-entreprises.data.gouv.fr/entreprise/349214825), [40](https://www.parrot.com/en), [41](https://www.parrot.com/us/drones/anafi/technical-specifications), [42](https://desolarplace.com/product/su-kam-40kva-360v-3-phase-colossal-inverter/), [43](https://citysolarmall.com/product/su-kam-50kva-360v-3-phase-colossal-inverter/), [44](https://www.parrot.com/us/drones/anafi-usa/technical-specifications), [45](https://www.manualslib.com/manual/1409899/Growatt-1500-Us.html?page=28), [46](https://www.parrot.com/us/drones/anafi-thermal/technical-specifications), [47](https://fr.linkedin.com/in/caroline-parot-72714252), [48](https://www.by-parot.com/), [49](https://avis-deces.linternaute.com/nom/famille-parot), [50](https://openreview.net/forum?id=Vx1JadlOIt), [51](https://huggingface.co/blog/vlms), [52](https://encord.com/blog/vision-language-models-guide/), [53](https://huggingface.co/blog/vlms-2025), [54](https://openreview.net/pdf?id=Vx1JadlOIt), [55](https://www.nature.com/articles/s41586-024-08378-w), [56](https://ieeexplore.ieee.org/document/10445007/), [57](https://www.nvidia.com/en-us/glossary/vision-language-models/), [58](https://arxiv.org/abs/2501.02189), [59](https://arxiv.org/abs/2304.00685), [60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3), [61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks), [62](https://arxiv.org/abs/2305.11175), [63](https://github.com/jingyi0000/VLM_survey), [64](https://www.ibm.com/think/topics/vision-language-models), [65](https://arxiv.org/html/2401.01862v1), [66](https://arxiv.org/abs/2505.11838), [67](https://escholarship.org/uc/item/3tc6256x), [68](https://paperswithcode.com/task/visual-reasoning/latest), [69](https://practicalpie.com/perceptual-reasoning/), [70](https://visualcommonsense.com/), [71](https://paperswithcode.com/task/visual-commonsense-reasoning), [72](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms), [73](https://nihtoolbox.org/test/visual-reasoning-test/), [74](https://www.continued.com/early-childhood-education/ask-the-experts/what-are-visual-perceptual-skills-22984), [75](http://brainy-child.com/experts/WISC-IV-perceptual-reasoning.shtml), [76](https://github.com/atfortes/Awesome-LLM-Reasoning), [77](https://iclr.cc/virtual/2025/calendar), [78](https://link.springer.com/article/10.1007/s10489-025-06325-4), [79](https://paperswithcode.com/dataset/vcr), [80](https://huggingface.co/tasks/visual-question-answering), [81](https://www.bloomberg.com/news/articles/2025-04-16/openai-releases-new-reasoning-models-for-coding-and-visual-tasks), [82](https://www.parrotanalytics.com/), [83](https://universe.roboflow.com/fish-zqrse/parrot-fish-nq63l), [84](https://arxiv.org/html/2407.14066v3), [85](https://www.marketscreener.com/quote/stock/GROUPE-PAROT-31670658/company/), [86](https://www.cpubenchmark.net/singleCompare.php?remove=3859), [87](https://360rumors.com/360-camera-comparison-tool/), [88](https://www.videocardbenchmark.net), [89](https://videocardbenchmark.net/singleCompare.php?redirect=), [90](https://cyberforge.academy/parrot-os-vs-kali-linux-the-ultimate-comparison-guide/), [91](https://www.lebloginsta360.com/en/blog/tags/360-degree-camera-comparison), [92](https://pitchbook.com/profiles/company/161768-35), [93](https://www.cpubenchmark.net/), [94](https://panoraven.com/blog/en/360-best-cameras/), [95](https://tweakers.net/best-buy-guide/processors/benchmarks), [96](https://www.parrot.com/en/apps-and-services), [97](https://x.com/pro360vision), [98](https://dl.acm.org/doi/10.1609/aaai.v37i3.25450), [99](https://deepai.org/publication/parot-a-practical-framework-for-robust-deep-neuralnetwork-training), [100](https://energyconnections.net.au/products/noark-dual-pole-dc-circuit-breaker-360v), [101](https://www.mouser.mx/fairchildpdpigbt/), [102](https://www.onworks.net/software/windows/app-parot-polar-aligner), [103](https://www.linkedin.com/in/john-bryan-parot-051a88133/), [104](https://www.academia.edu/42057304), [105](https://dl.acm.org/doi/abs/10.1016/j.eswa.2021.114712), [106](https://www.dronenerds.com/collections/drones-enterprise-drones-parrot-business), [107](https://www.sciencedirect.com/science/article/pii/S2590188520300032), [108](https://www.parrotsoftware.com/), [109](https://fr.wikipedia.org/wiki/Parrot_(entreprise)), [110](https://partav.com/), [111](https://www.usine-digitale.fr/parrot/)]. The PARROT-360V benchmark addresses the limitations of existing benchmarks by focusing on complex visual reasoning tasks [[86](https://www.cpubenchmark.net/singleCompare.php?remove=3859)][[87](https://360rumors.com/360-camera-comparison-tool/)][[88](https://www.videocardbenchmark.net)][[89](https://videocardbenchmark.net/singleCompare.php?redirect=)][[90](https://cyberforge.academy/parrot-os-vs-kali-linux-the-ultimate-comparison-guide/)][[91](https://www.lebloginsta360.com/en/blog/tags/360-degree-camera-comparison)][[92](https://pitchbook.com/profiles/company/161768-35)][[93](https://www.cpubenchmark.net/)][[94](https://panoraven.com/blog/en/360-best-cameras/)][[95](https://tweakers.net/best-buy-guide/processors/benchmarks)]. It uses visual puzzles to evaluate VLMs [[11](https://www.benchmarksixsigma.com/calculators/)][[12](https://www.bequiet.com/en/press/38261)][[13](https://www.pcbenchmarks.net/builder.php?cpuid=6355)][[14](https://archive.techplayboy.com/component/option,com_frontpage/Itemid,1/limit,9/limitstart,774/)][[15](https://en.wikipedia.org/wiki/Statistical_benchmarking)][[16](https://pypi.org/project/ai-benchmark/)][[17](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-benchmarks)][[18](https://github.com/bheisler/criterion.rs)]. Visual reasoning tasks are a key component of the PARROT-360V benchmark [[66](https://arxiv.org/abs/2505.11838)][[67](https://escholarship.org/uc/item/3tc6256x)][[68](https://paperswithcode.com/task/visual-reasoning/latest)][[69](https://practicalpie.com/perceptual-reasoning/)][[70](https://visualcommonsense.com/)][[71](https://paperswithcode.com/task/visual-commonsense-reasoning)][[72](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms)][[73](https://nihtoolbox.org/test/visual-reasoning-test/)][[74](https://www.continued.com/early-childhood-education/ask-the-experts/what-are-visual-perceptual-skills-22984)][[75](http://brainy-child.com/experts/WISC-IV-perceptual-reasoning.shtml)][[76](https://github.com/atfortes/Awesome-LLM-Reasoning)][[77](https://iclr.cc/virtual/2025/calendar)][[78](https://link.springer.com/article/10.1007/s10489-025-06325-4)][[79](https://paperswithcode.com/dataset/vcr)][[80](https://huggingface.co/tasks/visual-question-answering)][[81](https://www.bloomberg.com/news/articles/2025-04-16/openai-releases-new-reasoning-models-for-coding-and-visual-tasks)]. The benchmark is designed to evaluate the ability of VLMs to understand and process complex visual and textual information [[82](https://www.parrotanalytics.com/)][[83](https://universe.roboflow.com/fish-zqrse/parrot-fish-nq63l)][[84](https://arxiv.org/html/2407.14066v3)][[85](https://www.marketscreener.com/quote/stock/GROUPE-PAROT-31670658/company/)]. There is no direct information about a product named "PAROT-360V" [[96](https://www.parrot.com/en/apps-and-services)][[97](https://x.com/pro360vision)][[98](https://dl.acm.org/doi/10.1609/aaai.v37i3.25450)][[99](https://deepai.org/publication/parot-a-practical-framework-for-robust-deep-neuralnetwork-training)][[100](https://energyconnections.net.au/products/noark-dual-pole-dc-circuit-breaker-360v)][[101](https://www.mouser.mx/fairchildpdpigbt/)][[102](https://www.onworks.net/software/windows/app-parot-polar-aligner)][[103](https://www.linkedin.com/in/john-bryan-parot-051a88133/)][[104](https://www.academia.edu/42057304)][[105](https://dl.acm.org/doi/abs/10.1016/j.eswa.2021.114712)][[106](https://www.dronenerds.com/collections/drones-enterprise-drones-parrot-business)][[107](https://www.sciencedirect.com/science/article/pii/S2590188520300032)][[108](https://www.parrotsoftware.com/)][[109](https://fr.wikipedia.org/wiki/Parrot_(entreprise))][[110](https://partav.com/)][[111](https://www.usine-digitale.fr/parrot/)].The PARROT-360V benchmark consists of 2,487 visual puzzles [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)]. These puzzles are designed to assess the VLMs' capabilities in visual reasoning [[66](https://arxiv.org/abs/2505.11838)][[67](https://escholarship.org/uc/item/3tc6256x)][[68](https://paperswithcode.com/task/visual-reasoning/latest)][[69](https://practicalpie.com/perceptual-reasoning/)][[70](https://visualcommonsense.com/)][[71](https://paperswithcode.com/task/visual-commonsense-reasoning)][[72](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms)][[73](https://nihtoolbox.org/test/visual-reasoning-test/)][[74](https://www.continued.com/early-childhood-education/ask-the-experts/what-are-visual-perceptual-skills-22984)][[75](http://brainy-child.com/experts/WISC-IV-perceptual-reasoning.shtml)][[76](https://github.com/atfortes/Awesome-LLM-Reasoning)][[77](https://iclr.cc/virtual/2025/calendar)][[78](https://link.springer.com/article/10.1007/s10489-025-06325-4)][[79](https://paperswithcode.com/dataset/vcr)][[80](https://huggingface.co/tasks/visual-question-answering)][[81](https://www.bloomberg.com/news/articles/2025-04-16/openai-releases-new-reasoning-models-for-coding-and-visual-tasks)]. The benchmark uses visual puzzles to evaluate VLMs [[11](https://www.benchmarksixsigma.com/calculators/)][[12](https://www.bequiet.com/en/press/38261)][[13](https://www.pcbenchmarks.net/builder.php?cpuid=6355)][[14](https://archive.techplayboy.com/component/option,com_frontpage/Itemid,1/limit,9/limitstart,774/)][[15](https://en.wikipedia.org/wiki/Statistical_benchmarking)][[16](https://pypi.org/project/ai-benchmark/)][[17](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-benchmarks)][[18](https://github.com/bheisler/criterion.rs)]. It is a novel tool for evaluating Vision Language Models (VLMs) by focusing on complex visual reasoning tasks [[86](https://www.cpubenchmark.net/singleCompare.php?remove=3859)][[87](https://360rumors.com/360-camera-comparison-tool/)][[88](https://www.videocardbenchmark.net)][[89](https://videocardbenchmark.net/singleCompare.php?redirect=)][[90](https://cyberforge.academy/parrot-os-vs-kali-linux-the-ultimate-comparison-guide/)][[91](https://www.lebloginsta360.com/en/blog/tags/360-degree-camera-comparison)][[92](https://pitchbook.com/profiles/company/161768-35)][[93](https://www.cpubenchmark.net/)][[94](https://panoraven.com/blog/en/360-best-cameras/)][[95](https://tweakers.net/best-buy-guide/processors/benchmarks)]. The PARROT-360V benchmark is designed to evaluate Vision Language Models (VLMs) in complex, real-world problem-solving scenarios [[50](https://openreview.net/forum?id=Vx1JadlOIt)][[51](https://huggingface.co/blog/vlms)][[52](https://encord.com/blog/vision-language-models-guide/)][[53](https://huggingface.co/blog/vlms-2025)][[54](https://openreview.net/pdf?id=Vx1JadlOIt)][[55](https://www.nature.com/articles/s41586-024-08378-w)][[56](https://ieeexplore.ieee.org/document/10445007/)][[57](https://www.nvidia.com/en-us/glossary/vision-language-models/)][[58](https://arxiv.org/abs/2501.02189)][[59](https://arxiv.org/abs/2304.00685)][[60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3)][[61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks)][[62](https://arxiv.org/abs/2305.11175)][[63](https://github.com/jingyi0000/VLM_survey)][[64](https://www.ibm.com/think/topics/vision-language-models)][[65](https://arxiv.org/html/2401.01862v1)].The PARROT-360V benchmark includes 2,487 visual puzzles designed to evaluate Vision Language Models (VLMs) on complex visual reasoning tasks [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)]. This benchmark is designed to assess VLMs in real-world problem-solving scenarios [[50](https://openreview.net/forum?id=Vx1JadlOIt)][[51](https://huggingface.co/blog/vlms)][[52](https://encord.com/blog/vision-language-models-guide/)][[53](https://huggingface.co/blog/vlms-2025)][[54](https://openreview.net/pdf?id=Vx1JadlOIt)][[55](https://www.nature.com/articles/s41586-024-08378-w)][[56](https://ieeexplore.ieee.org/document/10445007/)][[57](https://www.nvidia.com/en-us/glossary/vision-language-models/)][[58](https://arxiv.org/abs/2501.02189)][[59](https://arxiv.org/abs/2304.00685)][[60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3)][[61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks)][[62](https://arxiv.org/abs/2305.11175)][[63](https://github.com/jingyi0000/VLM_survey)][[64](https://www.ibm.com/think/topics/vision-language-models)][[65](https://arxiv.org/html/2401.01862v1)]. By using visual puzzles, PARROT-360V evaluates the performance of VLMs [[11](https://www.benchmarksixsigma.com/calculators/)][[12](https://www.bequiet.com/en/press/38261)][[13](https://www.pcbenchmarks.net/builder.php?cpuid=6355)][[14](https://archive.techplayboy.com/component/option,com_frontpage/Itemid,1/limit,9/limitstart,774/)][[15](https://en.wikipedia.org/wiki/Statistical_benchmarking)][[16](https://pypi.org/project/ai-benchmark/)][[17](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-benchmarks)][[18](https://github.com/bheisler/criterion.rs)]. It is a novel tool that focuses on complex visual reasoning tasks [[66](https://arxiv.org/abs/2505.11838)][[67](https://escholarship.org/uc/item/3tc6256x)][[68](https://paperswithcode.com/task/visual-reasoning/latest)][[69](https://practicalpie.com/perceptual-reasoning/)][[70](https://visualcommonsense.com/)][[71](https://paperswithcode.com/task/visual-commonsense-reasoning)][[72](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms)][[73](https://nihtoolbox.org/test/visual-reasoning-test/)][[74](https://www.continued.com/early-childhood-education/ask-the-experts/what-are-visual-perceptual-skills-22984)][[75](http://brainy-child.com/experts/WISC-IV-perceptual-reasoning.shtml)][[76](https://github.com/atfortes/Awesome-LLM-Reasoning)][[77](https://iclr.cc/virtual/2025/calendar)][[78](https://link.springer.com/article/10.1007/s10489-025-06325-4)][[79](https://paperswithcode.com/dataset/vcr)][[80](https://huggingface.co/tasks/visual-question-answering)][[81](https://www.bloomberg.com/news/articles/2025-04-16/openai-releases-new-reasoning-models-for-coding-and-visual-tasks)] and addresses the limitations of existing benchmarks [[86](https://www.cpubenchmark.net/singleCompare.php?remove=3859)][[87](https://360rumors.com/360-camera-comparison-tool/)][[88](https://www.videocardbenchmark.net)][[89](https://videocardbenchmark.net/singleCompare.php?redirect=)][[90](https://cyberforge.academy/parrot-os-vs-kali-linux-the-ultimate-comparison-guide/)][[91](https://www.lebloginsta360.com/en/blog/tags/360-degree-camera-comparison)][[92](https://pitchbook.com/profiles/company/161768-35)][[93](https://www.cpubenchmark.net/)][[94](https://panoraven.com/blog/en/360-best-cameras/)][[95](https://tweakers.net/best-buy-guide/processors/benchmarks)]. The PARROT-360V benchmark assesses the ability of VLMs to understand and process complex visual and textual information [[82](https://www.parrotanalytics.com/)][[83](https://universe.roboflow.com/fish-zqrse/parrot-fish-nq63l)][[84](https://arxiv.org/html/2407.14066v3)][[85](https://www.marketscreener.com/quote/stock/GROUPE-PAROT-31670658/company/)]. However, there is no direct information about a product named "PAROT-360V" [[96](https://www.parrot.com/en/apps-and-services)][[97](https://x.com/pro360vision)][[98](https://dl.acm.org/doi/10.1609/aaai.v37i3.25450)][[99](https://deepai.org/publication/parot-a-practical-framework-for-robust-deep-neuralnetwork-training)][[100](https://energyconnections.net.au/products/noark-dual-pole-dc-circuit-breaker-360v)][[101](https://www.mouser.mx/fairchildpdpigbt/)][[102](https://www.onworks.net/software/windows/app-parot-polar-aligner)][[103](https://www.linkedin.com/in/john-bryan-parot-051a88133/)][[104](https://www.academia.edu/42057304)][[105](https://dl.acm.org/doi/abs/10.1016/j.eswa.2021.114712)][[106](https://www.dronenerds.com/collections/drones-enterprise-drones-parrot-business)][[107](https://www.sciencedirect.com/science/article/pii/S2590188520300032)][[108](https://www.parrotsoftware.com/)][[109](https://fr.wikipedia.org/wiki/Parrot_(entreprise))][[110](https://partav.com/)][[111](https://www.usine-digitale.fr/parrot/)].The PARROT-360V benchmark includes 2,487 visual puzzles specifically designed to evaluate the complex visual reasoning capabilities of Vision Language Models (VLMs) [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)]. These puzzles serve as samples within the benchmark, challenging VLMs to process and understand intricate visual and textual data [[82](https://www.parrotanalytics.com/)][[83](https://universe.roboflow.com/fish-zqrse/parrot-fish-nq63l)][[84](https://arxiv.org/html/2407.14066v3)][[85](https://www.marketscreener.com/quote/stock/GROUPE-PAROT-31670658/company/)]. By utilizing these visual puzzles, PARROT-360V offers a structured approach to assess VLM performance in real-world problem-solving scenarios [[50](https://openreview.net/forum?id=Vx1JadlOIt)][[51](https://huggingface.co/blog/vlms)][[52](https://encord.com/blog/vision-language-models-guide/)][[53](https://huggingface.co/blog/vlms-2025)][[54](https://openreview.net/pdf?id=Vx1JadlOIt)][[55](https://www.nature.com/articles/s41586-024-08378-w)][[56](https://ieeexplore.ieee.org/document/10445007/)][[57](https://www.nvidia.com/en-us/glossary/vision-language-models/)][[58](https://arxiv.org/abs/2501.02189)][[59](https://arxiv.org/abs/2304.00685)][[60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3)][[61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks)][[62](https://arxiv.org/abs/2305.11175)][[63](https://github.com/jingyi0000/VLM_survey)][[64](https://www.ibm.com/think/topics/vision-language-models)][[65](https://arxiv.org/html/2401.01862v1)], focusing on complex visual reasoning tasks [[66](https://arxiv.org/abs/2505.11838)][[67](https://escholarship.org/uc/item/3tc6256x)][[68](https://paperswithcode.com/task/visual-reasoning/latest)][[69](https://practicalpie.com/perceptual-reasoning/)][[70](https://visualcommonsense.com/)][[71](https://paperswithcode.com/task/visual-commonsense-reasoning)][[72](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms)][[73](https://nihtoolbox.org/test/visual-reasoning-test/)][[74](https://www.continued.com/early-childhood-education/ask-the-experts/what-are-visual-perceptual-skills-22984)][[75](http://brainy-child.com/experts/WISC-IV-perceptual-reasoning.shtml)][[76](https://github.com/atfortes/Awesome-LLM-Reasoning)][[77](https://iclr.cc/virtual/2025/calendar)][[78](https://link.springer.com/article/10.1007/s10489-025-06325-4)][[79](https://paperswithcode.com/dataset/vcr)][[80](https://huggingface.co/tasks/visual-question-answering)][[81](https://www.bloomberg.com/news/articles/2025-04-16/openai-releases-new-reasoning-models-for-coding-and-visual-tasks)] and addressing the limitations found in other benchmarks [[86](https://www.cpubenchmark.net/singleCompare.php?remove=3859)][[87](https://360rumors.com/360-camera-comparison-tool/)][[88](https://www.videocardbenchmark.net)][[89](https://videocardbenchmark.net/singleCompare.php?redirect=)][[90](https://cyberforge.academy/parrot-os-vs-kali-linux-the-ultimate-comparison-guide/)][[91](https://www.lebloginsta360.com/en/blog/tags/360-degree-camera-comparison)][[92](https://pitchbook.com/profiles/company/161768-35)][[93](https://www.cpubenchmark.net/)][[94](https://panoraven.com/blog/en/360-best-cameras/)][[95](https://tweakers.net/best-buy-guide/processors/benchmarks)].The PARROT-360V benchmark utilizes visual puzzles to evaluate VLMs [[11](https://www.benchmarksixsigma.com/calculators/)][[12](https://www.bequiet.com/en/press/38261)][[13](https://www.pcbenchmarks.net/builder.php?cpuid=6355)][[14](https://archive.techplayboy.com/component/option,com_frontpage/Itemid,1/limit,9/limitstart,774/)][[15](https://en.wikipedia.org/wiki/Statistical_benchmarking)][[16](https://pypi.org/project/ai-benchmark/)][[17](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-benchmarks)][[18](https://github.com/bheisler/criterion.rs)]. It is designed to evaluate VLMs in complex, real-world problem-solving scenarios [[50](https://openreview.net/forum?id=Vx1JadlOIt)][[51](https://huggingface.co/blog/vlms)][[52](https://encord.com/blog/vision-language-models-guide/)][[53](https://huggingface.co/blog/vlms-2025)][[54](https://openreview.net/pdf?id=Vx1JadlOIt)][[55](https://www.nature.com/articles/s41586-024-08378-w)][[56](https://ieeexplore.ieee.org/document/10445007/)][[57](https://www.nvidia.com/en-us/glossary/vision-language-models/)][[58](https://arxiv.org/abs/2501.02189)][[59](https://arxiv.org/abs/2304.00685)][[60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3)][[61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks)][[62](https://arxiv.org/abs/2305.11175)][[63](https://github.com/jingyi0000/VLM_survey)][[64](https://www.ibm.com/think/topics/vision-language-models)][[65](https://arxiv.org/html/2401.01862v1)]. The PARROT-360V benchmark includes 2,487 visual puzzles designed to evaluate Vision Language Models (VLMs) on complex visual reasoning tasks [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)]. This benchmark is designed to assess VLMs in real-world problem-solving scenarios [[50](https://openreview.net/forum?id=Vx1JadlOIt)][[51](https://huggingface.co/blog/vlms)][[52](https://encord.com/blog/vision-language-models-guide/)][[53](https://huggingface.co/blog/vlms-2025)][[54](https://openreview.net/pdf?id=Vx1JadlOIt)][[55](https://www.nature.com/articles/s41586-024-08378-w)][[56](https://ieeexplore.ieee.org/document/10445007/)][[57](https://www.nvidia.com/en-us/glossary/vision-language-models/)][[58](https://arxiv.org/abs/2501.02189)][[59](https://arxiv.org/abs/2304.00685)][[60](https://ritvik19.medium.com/papers-explained-an-introduction-to-vision-language-modeling-89e7697da6e3)][[61](https://www.researchgate.net/publication/370870034_VisionLLM_Large_Language_Model_is_also_an_Open-Ended_Decoder_for_Vision-Centric_Tasks)][[62](https://arxiv.org/abs/2305.11175)][[63](https://github.com/jingyi0000/VLM_survey)][[64](https://www.ibm.com/think/topics/vision-language-models)][[65](https://arxiv.org/html/2401.01862v1)]. By using visual puzzles, PARROT-360V evaluates the performance of VLMs [[11](https://www.benchmarksixsigma.com/calculators/)][[12](https://www.bequiet.com/en/press/38261)][[13](https://www.pcbenchmarks.net/builder.php?cpuid=6355)][[14](https://archive.techplayboy.com/component/option,com_frontpage/Itemid,1/limit,9/limitstart,774/)][[15](https://en.wikipedia.org/wiki/Statistical_benchmarking)][[16](https://pypi.org/project/ai-benchmark/)][[17](https://learn.microsoft.com/en-us/azure/virtual-machines/disks-benchmarks)][[18](https://github.com/bheisler/criterion.rs)]. It is a novel tool that focuses on complex visual reasoning tasks [[66](https://arxiv.org/abs/2505.11838)][[67](https://escholarship.org/uc/item/3tc6256x)][[68](https://paperswithcode.com/task/visual-reasoning/latest)][[69](https://practicalpie.com/perceptual-reasoning/)][[70](https://visualcommonsense.com/)][[71](https://paperswithcode.com/task/visual-commonsense-reasoning)][[72](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms)][[73](https://nihtoolbox.org/test/visual-reasoning-test/)][[74](https://www.continued.com/early-childhood-education/ask-the-experts/what-are-visual-perceptual-skills-22984)][[75](http://brainy-child.com/experts/WISC-IV-perceptual-reasoning.shtml)][[76](https://github.com/atfortes/Awesome-LLM-Reasoning)][[77](https://iclr.cc/virtual/2025/calendar)][[78](https://link.springer.com/article/10.1007/s10489-025-06325-4)][[79](https://paperswithcode.com/dataset/vcr)][[80](https://huggingface.co/tasks/visual-question-answering)][[81](https://www.bloomberg.com/news/articles/2025-04-16/openai-releases-new-reasoning-models-for-coding-and-visual-tasks)] and addresses the limitations of existing benchmarks [[86](https://www.cpubenchmark.net/singleCompare.php?remove=3859)][[87](https://360rumors.com/360-camera-comparison-tool/)][[88](https://www.videocardbenchmark.net)][[89](https://videocardbenchmark.net/singleCompare.php?redirect=)][[90](https://cyberforge.academy/parrot-os-vs-kali-linux-the-ultimate-comparison-guide/)][[91](https://www.lebloginsta360.com/en/blog/tags/360-degree-camera-comparison)][[92](https://pitchbook.com/profiles/company/161768-35)][[93](https://www.cpubenchmark.net/)][[94](https://panoraven.com/blog/en/360-best-cameras/)][[95](https://tweakers.net/best-buy-guide/processors/benchmarks)]. The PARROT-360V benchmark assesses the ability of VLMs to understand and process complex visual and textual information [[82](https://www.parrotanalytics.com/)][[83](https://universe.roboflow.com/fish-zqrse/parrot-fish-nq63l)][[84](https://arxiv.org/html/2407.14066v3)][[85](https://www.marketscreener.com/quote/stock/GROUPE-PAROT-31670658/company/)]. However, there is no direct information about a product named "PAROT-360V" [[96](https://www.parrot.com/en/apps-and-services)][[97](https://x.com/pro360vision)][[98](https://dl.acm.org/doi/10.1609/aaai.v37i3.25450)][[99](https://deepai.org/publication/parot-a-practical-framework-for-robust-deep-neuralnetwork-training)][[100](https://energyconnections.net.au/products/noark-dual-pole-dc-circuit-breaker-360v)][[101](https://www.mouser.mx/fairchildpdpigbt/)][[102](https://www.onworks.net/software/windows/app-parot-polar-aligner)][[103](https://www.linkedin.com/in/john-bryan-parot-051a88133/)][[104](https://www.academia.edu/42057304)][[105](https://dl.acm.org/doi/abs/10.1016/j.eswa.2021.114712)][[106](https://www.dronenerds.com/collections/drones-enterprise-drones-parrot-business)][[107](https://www.sciencedirect.com/science/article/pii/S2590188520300032)][[108](https://www.parrotsoftware.com/)][[109](https://fr.wikipedia.org/wiki/Parrot_(entreprise))][[110](https://partav.com/)][[111](https://www.usine-digitale.fr/parrot/)].The PARROT-360V benchmark includes 2,487 visual puzzles designed to evaluate Vision Language Models (VLMs) on complex visual reasoning tasks [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)]. These puzzles serve as samples to assess VLM performance [[1](https://arxiv.org/abs/2411.15201)][[2](https://arxiv.org/html/2411.15201v1)][[3](https://aclanthology.org/2025.coling-industry.6/)][[4](https://www.redblock.ai/resources/blog/parrot-360v-elevating-vision-language-model-evaluation-through-real-world-problem-solving)].
✨  Done in 205.51s.